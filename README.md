# **preciosCABA: API de AnÃ¡lisis y PredicciÃ³n de Precios Inmobiliarios**

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.3+-orange.svg)](https://scikit-learn.org)
[![MySQL](https://img.shields.io/badge/MySQL-8.0+-blue.svg)](https://mysql.com)

## ğŸ  **Â¿QuÃ© es preciosCABA?**

Una API completa de anÃ¡lisis inmobiliario que procesa datos de propiedades en la Ciudad de Buenos Aires y ofrece:

- **ğŸ“Š AnÃ¡lisis estadÃ­stico** del mercado inmobiliario
- **ğŸ¤– Predicciones de precios** con intervalos de confianza
- **ğŸ“ˆ Visualizaciones** y insights de mercado
- **ğŸ” Consultas avanzadas** con filtros y paginaciÃ³n

## ğŸš€ **Inicio RÃ¡pido**

```bash
# 1. Clonar el repositorio
git clone <tu-repo>
cd api-analisis-inmobiliario-caba/api

# 2. Instalar dependencias
pip install -r requirements.txt

# 3. Configurar base de datos (ver docs/inicio-rapido.md)
cp .env.example .env

# 4. Poblar la base de datos
python scripts/poblar_db.py

# 5. Entrenar el modelo
jupyter notebook notebooks/entrenamiento_modelo.ipynb

# 6. Iniciar la API
uvicorn src.api.main:app --reload
```

**ğŸŒ Acceder a la documentaciÃ³n:** http://127.0.0.1:8000/docs

## ğŸ“š **DocumentaciÃ³n Completa**

- **[ğŸš€ Inicio RÃ¡pido](docs/inicio-rapido.md)** - InstalaciÃ³n y configuraciÃ³n paso a paso
- **[ğŸ“– Referencia de API](docs/referencia-api.md)** - DocumentaciÃ³n completa de endpoints
- **[ğŸ¤– Modelo de ML](docs/modelo-ml.md)** - Detalles del modelo de Machine Learning
- **[ğŸ—ï¸ Arquitectura](docs/arquitectura.md)** - DiseÃ±o tÃ©cnico y decisiones
- **[ğŸ“Š Visualizaciones](docs/visualizaciones.md)** - GrÃ¡ficos y anÃ¡lisis estadÃ­sticos
- **[ğŸ’¡ Ejemplos](docs/ejemplos.md)** - Casos de uso y ejemplos prÃ¡cticos

## ğŸ¯ **CaracterÃ­sticas Principales**

### **Machine Learning Avanzado**
- âœ… Modelo RandomForest con RÂ² = 0.87
- âœ… Intervalos de confianza del 95%
- âœ… AnÃ¡lisis de propiedades similares
- âœ… Feature importance y metadata del modelo

### **API REST Completa**
- âœ… CRUD completo de propiedades
- âœ… Filtros avanzados y paginaciÃ³n
- âœ… EstadÃ­sticas por barrio y evoluciÃ³n temporal
- âœ… Validaciones estrictas de input

### **AnÃ¡lisis de Datos**
- âœ… ETL robusto con limpieza de datos
- âœ… 50,000+ registros procesados
- âœ… Visualizaciones interactivas
- âœ… Insights de mercado en tiempo real

## ğŸ› ï¸ **TecnologÃ­as Utilizadas**

- **Backend:** FastAPI, Pydantic, MySQL
- **Machine Learning:** Scikit-learn, Pandas, NumPy
- **AnÃ¡lisis:** Jupyter Notebooks, Matplotlib, Seaborn
- **Infraestructura:** Docker-ready, Logging estructurado

## ğŸ“Š **Ejemplo de PredicciÃ³n**

```json
POST /predict/
{
  "barrio": "Palermo",
  "ambientes": 3,
  "dormitorios": 2,
  "banos": 2,
  "superficie_total_m2": 80,
  "cocheras": 1
}
```

**Respuesta:**
```json
{
  "predicted_price_usd": 185000.0,
  "confidence_interval": {"lower": 170000, "upper": 200000},
  "similar_properties_avg": 178500.0
}
```

## ğŸ“ˆ **Estado del Proyecto**

### **âœ… Completado**
- [x] AnÃ¡lisis exploratorio de datos (EDA)
- [x] Pipeline ETL completo con limpieza de datos
- [x] API REST con CRUD completo
- [x] Modelo de ML con anÃ¡lisis avanzado
- [x] Validaciones estrictas y logging
- [x] DocumentaciÃ³n tÃ©cnica completa

### **ğŸš€ En Desarrollo**
- [ ] Feature engineering con NLP
- [ ] Dashboard interactivo con Streamlit
- [ ] ValidaciÃ³n cruzada y mÃ©tricas adicionales

## ğŸ‘¨â€ğŸ’» **Sobre el Proyecto**

Este proyecto forma parte de mi portfolio personal como estudiante de programaciÃ³n orientado a **Ciencia de Datos e Inteligencia Artificial**.

**TecnologÃ­as demostradas:**
- ğŸ”§ **IngenierÃ­a de Datos:** ETL, limpieza, validaciÃ³n
- ğŸ¤– **Machine Learning:** Modelado, evaluaciÃ³n, despliegue
- ğŸŒ **Desarrollo Backend:** APIs REST, bases de datos
- ğŸ“Š **AnÃ¡lisis de Datos:** EDA, visualizaciones, insights

**Contacto:** joaquin99911@gmail.com

---

*Este proyecto demuestra un ciclo completo de ciencia de datos, desde el anÃ¡lisis exploratorio hasta el despliegue de un modelo de ML en producciÃ³n, siguiendo las mejores prÃ¡cticas de la industria.*