# **Arquitectura y Decisiones T√©cnicas**

Documentaci√≥n t√©cnica del dise√±o del sistema, decisiones arquitect√≥nicas y mejores pr√°cticas implementadas.

## **Visi√≥n General de la Arquitectura**

El proyecto sigue una arquitectura de **microservicios modulares** con separaci√≥n clara de responsabilidades:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ETL Pipeline  ‚îÇ    ‚îÇ   ML Pipeline   ‚îÇ    ‚îÇ   API REST      ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ Data Cleaning ‚îÇ    ‚îÇ ‚Ä¢ Model Training‚îÇ    ‚îÇ ‚Ä¢ FastAPI       ‚îÇ
‚îÇ ‚Ä¢ Validation    ‚îÇ    ‚îÇ ‚Ä¢ Prediction    ‚îÇ    ‚îÇ ‚Ä¢ Pydantic      ‚îÇ
‚îÇ ‚Ä¢ MySQL Load    ‚îÇ    ‚îÇ ‚Ä¢ Persistence   ‚îÇ    ‚îÇ ‚Ä¢ Endpoints     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   MySQL DB      ‚îÇ
                    ‚îÇ                 ‚îÇ
                    ‚îÇ ‚Ä¢ Properties    ‚îÇ
                    ‚îÇ ‚Ä¢ Clean Data    ‚îÇ
                    ‚îÇ ‚Ä¢ Indexes       ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## **Estructura del Proyecto**

```
api/
‚îú‚îÄ‚îÄ data/                          # Datos crudos
‚îÇ   ‚îî‚îÄ‚îÄ ventas_deptos.pkl
‚îú‚îÄ‚îÄ docs/                          # Documentaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ inicio-rapido.md
‚îÇ   ‚îú‚îÄ‚îÄ referencia-api.md
‚îÇ   ‚îú‚îÄ‚îÄ modelo-ml.md
‚îÇ   ‚îú‚îÄ‚îÄ arquitectura.md
‚îÇ   ‚îú‚îÄ‚îÄ visualizaciones.md
‚îÇ   ‚îî‚îÄ‚îÄ ejemplos.md
‚îú‚îÄ‚îÄ notebooks/                     # An√°lisis y entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ EDA.ipynb                 # An√°lisis exploratorio
‚îÇ   ‚îî‚îÄ‚îÄ entrenamiento_modelo.ipynb # Entrenamiento del modelo
‚îú‚îÄ‚îÄ scripts/                       # Scripts de ETL
‚îÇ   ‚îî‚îÄ‚îÄ poblar_db.py
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/                       # API REST
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py               # Aplicaci√≥n principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db_connection.py      # Conexi√≥n a BD
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas.py            # Modelos Pydantic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routers/              # Endpoints
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ propiedades.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ predictions.py
‚îÇ   ‚îî‚îÄ‚îÄ ml/                       # Modelo de ML
‚îÇ       ‚îú‚îÄ‚îÄ feature_engineering.py # Extracci√≥n de features NLP
‚îÇ       ‚îú‚îÄ‚îÄ model.pkl             # Modelo entrenado
‚îÇ       ‚îú‚îÄ‚îÄ model_columns.pkl     # Columnas del modelo
‚îÇ       ‚îú‚îÄ‚îÄ predict.py           # Funci√≥n de predicci√≥n
‚îÇ       ‚îî‚îÄ‚îÄ README.md            # Documentaci√≥n del modelo
‚îú‚îÄ‚îÄ requirements.txt              # Dependencias
‚îî‚îÄ‚îÄ README.md                     # Archivo principal
```

## **Flujo de Datos (Data Pipeline)**

### **1. Fase de Extracci√≥n**
- **Fuente:** Dataset crudo en formato `.pkl`
- **Contenido:** 50,691 registros de propiedades
- **Calidad:** Datos sucios con problemas de formato

### **2. Fase de Transformaci√≥n**
```python
# Proceso ETL implementado en scripts/poblar_db.py
def transformar_datos(df: pd.DataFrame) -> pd.DataFrame:
    # 1. Limpieza de precios
    df['price_usd'] = df['Price'].apply(limpiar_y_validar_precio)
    
    # 2. Estandarizaci√≥n de barrios
    df['barrio'] = df['Location'].apply(estandarizar_barrio)
    
    # 3. Parsing de caracter√≠sticas
    features_df = df['Features'].apply(parsear_features)
    
    # 4. Imputaci√≥n de valores faltantes
    df['expensas_ars'] = df['expensas_ars'].fillna(mediana_expensas)
    
    return df
```

### **3. Fase de Carga**
- **Destino:** Base de datos MySQL
- **Tabla:** `propiedades` con √≠ndices optimizados
- **Validaci√≥n:** Constraints y tipos de datos estrictos

## üóÑÔ∏è **Dise√±o de Base de Datos**

### **Esquema de la Tabla `propiedades`**
```sql
CREATE TABLE propiedades (
    id INT AUTO_INCREMENT PRIMARY KEY,
    source_id VARCHAR(255) UNIQUE NOT NULL,
    price_usd DECIMAL(12,2),
    expensas_ars DECIMAL(10,2),
    barrio VARCHAR(100) NOT NULL,
    address TEXT,
    ambientes INT,
    dormitorios INT,
    banos INT,
    superficie_total_m2 INT,
    cocheras INT,
    description TEXT,
    link VARCHAR(500),
    scrap_date DATETIME,
    
    -- √çndices para optimizaci√≥n
    INDEX idx_barrio (barrio),
    INDEX idx_price (price_usd),
    INDEX idx_superficie (superficie_total_m2),
    INDEX idx_scrap_date (scrap_date)
);
```

### **Decisiones de Dise√±o**
- **Tipos de datos:** DECIMAL para precios (precisi√≥n monetaria)
- **√çndices:** Optimizaci√≥n para consultas frecuentes
- **Constraints:** UNIQUE en source_id para evitar duplicados
- **Normalizaci√≥n:** Tabla √∫nica para simplicidad (no hay redundancia significativa)

## üåê **Arquitectura de la API**

### **Framework: FastAPI**
**Justificaci√≥n:**
- **Performance:** Una de las APIs m√°s r√°pidas de Python
- **Documentaci√≥n autom√°tica:** Swagger/OpenAPI integrado
- **Type hints:** Validaci√≥n autom√°tica con Pydantic
- **Async support:** Preparado para escalabilidad

### **Estructura de Routers**
```python
# Separaci√≥n por dominio de negocio
app.include_router(propiedades.router, prefix="/propiedades", tags=["Propiedades"])
app.include_router(predictions.router, prefix="/predict", tags=["Predicciones"])
```

### **Gesti√≥n de Conexiones**
```python
# Pool de conexiones para eficiencia
connection_pool = mysql.connector.pooling.MySQLConnectionPool(
    pool_name="propiedades_pool",
    pool_size=5,
    host=os.getenv("DB_HOST"),
    user=os.getenv("DB_USER"),
    password=os.getenv("DB_PASSWORD"),
    database=os.getenv("DB_NAME")
)
```

## Arquitectura de Machine Learning

### Pipeline de ML
```
Datos Crudos ‚Üí Limpieza ‚Üí NLP Feature Engineering ‚Üí Entrenamiento ‚Üí Persistencia ‚Üí Predicci√≥n
     ‚îÇ              ‚îÇ              ‚îÇ                   ‚îÇ            ‚îÇ              ‚îÇ
     ‚îÇ              ‚îÇ              ‚îÇ                   ‚îÇ            ‚îÇ              ‚îÇ
  MySQL DB    Preprocessing   TF-IDF Vectorization   XGBoost      Pickle       API Endpoint
```

### Separaci√≥n de Responsabilidades
- **`notebooks/entrenamiento_modelo.ipynb`:** Entrenamiento, comparaci√≥n y optimizaci√≥n de modelos.
- **`src/ml/feature_engineering.py`:** L√≥gica para la vectorizaci√≥n TF-IDF.
- **`src/ml/predict.py`:** L√≥gica de predicci√≥n y explicabilidad (SHAP).
- **`src/ml/model.pkl`:** Modelo serializado (XGBoost optimizado).
- **`src/ml/tfidf_vectorizer.pkl`:** Vectorizador TF-IDF entrenado.
- **`src/ml/model_columns.pkl`:** Metadatos de las columnas del modelo.

### Decisiones de Modelado
- **Algoritmo:** XGBoost
- **Justificaci√≥n:** Rendimiento superior demostrado tras optimizaci√≥n de hiperpar√°metros.
- **Features:** 156 caracter√≠sticas (5 num√©ricas + 51 de barrios + 100 de TF-IDF).
- **Validaci√≥n:** K-Fold Cross-Validation (k=5) y `RandomizedSearchCV` para optimizaci√≥n.

## üîí **Seguridad y Configuraci√≥n**

### **Gesti√≥n de Secretos**
```python
# Variables de entorno con python-dotenv
load_dotenv()
DB_HOST = os.getenv("DB_HOST")
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")
DB_NAME = os.getenv("DB_NAME")
```

### **Validaci√≥n de Input**
```python
# Validaciones estrictas con Pydantic
class PredictionInput(BaseModel):
    barrio: str = Field(..., description="Barrio de CABA")
    ambientes: int = Field(..., ge=1, le=10)
    # ... m√°s validaciones
    
    @field_validator('barrio')
    @classmethod
    def validate_barrio(cls, v):
        # Validaci√≥n personalizada
```

## üìä **Monitoreo y Logging**

### **Sistema de Logging**
```python
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Log de predicciones para an√°lisis posterior
logger.info(
    f"Predicci√≥n realizada: {prediction:.2f} USD | "
    f"Barrio: {data.get('barrio')} | "
    f"Superficie: {data.get('superficie_total_m2')} m¬≤"
)
```

### **M√©tricas de Monitoreo**
- **Performance:** Tiempo de respuesta de endpoints
- **Calidad:** Logs de predicciones y errores
- **Uso:** Frecuencia de consultas por endpoint
- **Errores:** Manejo de excepciones y c√≥digos de estado

## üöÄ **Escalabilidad y Performance**

### **Optimizaciones Implementadas**
- **Pool de conexiones:** Reutilizaci√≥n de conexiones DB
- **√çndices de BD:** Consultas optimizadas
- **Caching:** Modelo cargado en memoria
- **Validaci√≥n temprana:** Pydantic valida antes de procesar

### **Puntos de Escalabilidad**
- **Horizontal:** M√∫ltiples instancias de la API
- **Vertical:** M√°s recursos para el servidor
- **Base de datos:** Read replicas para consultas
- **Caching:** Redis para consultas frecuentes

## Herramientas y Tecnolog√≠as

### Stack Tecnol√≥gico
- **Backend:** FastAPI, Pydantic, Uvicorn
- **Base de datos:** MySQL 8.0
- **Machine Learning:** Scikit-learn, XGBoost, SHAP, Pandas, NumPy
- **An√°lisis:** Jupyter Notebooks, Matplotlib, Seaborn
- **Testing:** Pytest
- **Despliegue:** Docker
- **Configuraci√≥n:** python-dotenv

## M√©tricas de Calidad

### C√≥digo
- **Cobertura de Tests:** Implementada para l√≥gica de ML y endpoints de API.
- **Linting:** Sin errores de linting
- **Type hints:** 100% de funciones tipadas
- **Documentaci√≥n:** Documentaci√≥n completa en c√≥digo y en `docs/`.

### Modelo de ML
- **R¬≤:** 0.914 (excelente)
- **RMSE:** ~$116,398 USD (contextualizado)
- **Validaci√≥n:** K-Fold Cross-Validation y RandomizedSearchCV
- **Persistencia:** Modelo y artefactos serializados correctamente

### API
- **Response time:** < 150ms para predicciones y explicaciones.
- **Availability:** 99.9% (sin dependencias externas)
- **Error rate:** < 1% (validaciones estrictas)
- **Documentation:** Swagger autom√°tico y documentaci√≥n manual.

## Roadmap de Mejoras

### Corto Plazo
- **CI/CD:** Pipeline con GitHub Actions para automatizar tests y builds de Docker.
- **Monitoring:** Integrar un dashboard para visualizar m√©tricas de la API en tiempo real (ej. Grafana).

### Mediano Plazo
- **Microservicios:** Separaci√≥n completa de ETL, ML y API.
- **MLOps:** MLflow para experiment tracking y registro de modelos.
- **Caching:** Redis para resultados de endpoints de estad√≠sticas.

### Largo Plazo
- **Real-time:** WebSocket para predicciones en tiempo real.
- **ML Pipeline:** Re-entrenamiento autom√°tico del modelo.
- **Analytics:** Dashboard interactivo para el usuario final (ej. Streamlit o Dash).

## üîó **Enlaces Relacionados**

- **[üöÄ Inicio R√°pido](inicio-rapido.md)** - Configuraci√≥n del entorno
- **[üìñ Referencia de API](referencia-api.md)** - Documentaci√≥n de endpoints
- **[ü§ñ Modelo de ML](modelo-ml.md)** - Detalles del modelo
- **[üìä Visualizaciones](visualizaciones.md)** - An√°lisis visual
- **[üí° Ejemplos](ejemplos.md)** - Casos de uso pr√°cticos
